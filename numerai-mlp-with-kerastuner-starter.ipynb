{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "SEED=2023\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "import pathlib\n",
    "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import tensorflow as tf\n",
    "!pip install numerapi\n",
    "import numerapi\n",
    "# import tensorflow_addons as tfa\n",
    "# !pip install -q -U keras-tuner\n",
    "!pip install keras-tuner\n",
    "import keras_tuner as kt # keras tuner!\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "sns.set_context(\"talk\")\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "data_directory = \"../kazutsugi/datasets/\"\n",
    "EXP_DIR =\"../example\"\n",
    "OUTPUT_DIR= \"../output\"\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numerapi in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (2.6.0)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2.8.2)\r\n",
      "Requirement already satisfied: pytz in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2021.1)\r\n",
      "Requirement already satisfied: pandas>=1.1.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>=4.29.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (4.62.0)\r\n",
      "Requirement already satisfied: click>=7.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (8.0.1)\r\n",
      "Requirement already satisfied: requests in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2.26.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from pandas>=1.1.0->numerapi) (1.19.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from python-dateutil->numerapi) (1.15.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (3.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (2021.5.30)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Requirement already satisfied: keras-tuner in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (1.0.4)\r\n",
      "Requirement already satisfied: numpy in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (1.19.5)\r\n",
      "Requirement already satisfied: kt-legacy in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (1.0.4)\r\n",
      "Requirement already satisfied: scipy in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (1.7.1)\r\n",
      "Requirement already satisfied: requests in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (2.26.0)\r\n",
      "Requirement already satisfied: packaging in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (21.0)\r\n",
      "Requirement already satisfied: ipython in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (7.26.0)\r\n",
      "Requirement already satisfied: tensorboard in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from keras-tuner) (2.7.0)\r\n",
      "Requirement already satisfied: pygments in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (2.9.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (3.0.19)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (0.18.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (4.8.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (0.1.2)\r\n",
      "Requirement already satisfied: pickleshare in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (0.7.5)\r\n",
      "Requirement already satisfied: backcall in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (0.2.0)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (57.0.0)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (5.0.5)\r\n",
      "Requirement already satisfied: appnope in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (0.1.2)\r\n",
      "Requirement already satisfied: decorator in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from ipython->keras-tuner) (5.0.9)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.2)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from pexpect>4.3->ipython->keras-tuner) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\r\n",
      "Requirement already satisfied: ipython-genutils in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from packaging->keras-tuner) (2.4.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->keras-tuner) (3.2)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->keras-tuner) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->keras-tuner) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->keras-tuner) (2021.5.30)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (0.4.6)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (3.3.4)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (0.36.2)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (1.41.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (1.8.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (2.3.0)\r\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (3.18.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (2.0.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (0.6.1)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from tensorboard->keras-tuner) (0.14.1)\r\n",
      "Requirement already satisfied: six in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard->keras-tuner) (1.15.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numerapi in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (2.6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.29.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (4.62.0)\r\n",
      "Requirement already satisfied: pytz in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2.8.2)\r\n",
      "Requirement already satisfied: pandas>=1.1.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (1.3.1)\r\n",
      "Requirement already satisfied: click>=7.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (8.0.1)\r\n",
      "Requirement already satisfied: requests in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from numerapi) (2.26.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from pandas>=1.1.0->numerapi) (1.19.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from python-dateutil->numerapi) (1.15.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (3.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (2021.5.30)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andrewjennings/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages (from requests->numerapi) (1.26.6)\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numerapi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config\n",
    "Here you can choose what you do: \n",
    "\n",
    "- Debug mode (using small proportion of data)?\n",
    "- Tuning or not?\n",
    "- Which target to predict?\n",
    "- Seed number?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class CFG:\n",
    "    DEBUG = False # debug option\n",
    "    TUNING = True # whether to use the KerasTuner or not\n",
    "\n",
    "    INPUT_FILE_PATH = '../input/numerai-train-validation-with-kazutsugi-nomi/numerai_training_validation_target_nomi.csv'\n",
    "    OUTPUT_DIR = ''\n",
    "    TARGET = 'target_nomi' # target_kazutsugi\n",
    "    SEED = 2021"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Logging is always nice for your experiment:)\n",
    "def init_logger(log_file=CFG.OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = init_logger()\n",
    "logger.info('Start Logging...')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start Logging...\n",
      "Start Logging...\n",
      "Start Logging...\n",
      "2021-10-16 15:20:43,565 INFO __main__: Start Logging...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "logger.info('DEBUG : {}'.format(CFG.DEBUG))\n",
    "logger.info('TUNING : {}'.format(CFG.TUNING))\n",
    "logger.info('TARGET : {}'.format(CFG.TARGET))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : False\n",
      "DEBUG : False\n",
      "DEBUG : False\n",
      "2021-10-16 15:20:43,572 INFO __main__: DEBUG : False\n",
      "TUNING : True\n",
      "TUNING : True\n",
      "TUNING : True\n",
      "2021-10-16 15:20:43,575 INFO __main__: TUNING : True\n",
      "TARGET : target_nomi\n",
      "TARGET : target_nomi\n",
      "TARGET : target_nomi\n",
      "2021-10-16 15:20:43,577 INFO __main__: TARGET : target_nomi\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data\n",
    "In this dataset, we have two targets: 'kazutsugi' (old one) and 'nomi' (new one)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def download_data():\n",
    "\n",
    "    napi = numerapi.NumerAPI(verbosity=\"info\")\n",
    "    data_archive = napi.download_current_dataset(dest_path='../tmp', unzip=False)\n",
    "\n",
    "    with zipfile.ZipFile(data_archive, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"../kazutsugi/datasets\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    download_data()\n",
    "\n",
    "    print(\"# Loading data...\")\n",
    "    # The training data is used to train your model how to predict the targets.\n",
    "    training_data = pd.read_csv(data_directory + \"numerai_training_data.csv\").set_index(\"id\")\n",
    "    # The tournament data is the data that Numerai uses to evaluate your model.\n",
    "    tournament_data = pd.read_csv(data_directory + \"numerai_tournament_data.csv\").set_index(\"id\")\n",
    "\n",
    "    feature_names = [ f for f in training_data.columns if f.startswith(\"feature\")]\n",
    "\n",
    "    print(f\"Loaded {len(feature_names)} features\")\n",
    "\n",
    "\n",
    "    print(\"Training model\")\n",
    "\n",
    "\n",
    "\n",
    "    return training_data,feature_names,tournament_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "def get_int(x):\n",
    "    try:\n",
    "        return int(x[3:])\n",
    "    except:\n",
    "        return 1000\n",
    "\n",
    "training_data,feature_names,tournament_data = get_data()\n",
    "validation_data = tournament_data[tournament_data.data_type == \"validation\"]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 15:20:44,937 INFO numerapi.utils: target file already exists\n",
      "2021-10-16 15:20:44,938 INFO numerapi.utils: resuming download\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m/var/folders/s4/hqpk71s51lq88685j836lbjw0000gn/T/ipykernel_83210/2497086552.py\u001B[0m in \u001B[0;36mget_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mdownload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"# Loading data...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/s4/hqpk71s51lq88685j836lbjw0000gn/T/ipykernel_83210/3935184626.py\u001B[0m in \u001B[0;36mdownload_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mnapi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumerapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNumerAPI\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mverbosity\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"info\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mdata_archive\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload_current_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdest_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'../tmp'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munzip\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mzipfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mZipFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_archive\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mzip_ref\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/numerapi/numerapi.py\u001B[0m in \u001B[0;36mdownload_current_dataset\u001B[0;34m(self, dest_path, dest_filename, unzip, tournament)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[0murl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_dataset_url\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtournament\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 112\u001B[0;31m         \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow_progress_bars\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0;31m# unzip dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/numerapi/utils.py\u001B[0m in \u001B[0;36mdownload_file\u001B[0;34m(url, dest_path, show_progress_bars)\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"resuming download\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m             \u001B[0mresume_header\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m'Range'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'bytes=%d-'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mfile_size\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m             req = requests.get(url, headers=resume_header, stream=True,\n\u001B[0m\u001B[1;32m     52\u001B[0m                                verify=False, allow_redirects=True)\n\u001B[1;32m     53\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mfile_size\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtotal_size\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/requests/api.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m     \"\"\"\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'get'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/requests/api.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    540\u001B[0m         }\n\u001B[1;32m    541\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 542\u001B[0;31m         \u001B[0mresp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    543\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    653\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    654\u001B[0m         \u001B[0;31m# Send the request\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 655\u001B[0;31m         \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    656\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    657\u001B[0m         \u001B[0;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/requests/adapters.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mchunked\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 439\u001B[0;31m                 resp = conn.urlopen(\n\u001B[0m\u001B[1;32m    440\u001B[0m                     \u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m                     \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    697\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    698\u001B[0m             \u001B[0;31m# Make the request on the httplib connection object.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 699\u001B[0;31m             httplib_response = self._make_request(\n\u001B[0m\u001B[1;32m    700\u001B[0m                 \u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m                 \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[0;31m# Trigger any extra validation we need to do.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_conn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    384\u001B[0m             \u001B[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1008\u001B[0m         \u001B[0;31m# Force connect early to allow us to validate the connection.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1009\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sock\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# AppEngine might not have  `.sock`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1010\u001B[0;31m             \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1011\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1012\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_verified\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36mconnect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    409\u001B[0m             \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_default_certs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 411\u001B[0;31m         self.sock = ssl_wrap_socket(\n\u001B[0m\u001B[1;32m    412\u001B[0m             \u001B[0msock\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    413\u001B[0m             \u001B[0mkeyfile\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkey_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001B[0m in \u001B[0;36mssl_wrap_socket\u001B[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msend_sni\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 449\u001B[0;31m         ssl_sock = _ssl_wrap_socket_impl(\n\u001B[0m\u001B[1;32m    450\u001B[0m             \u001B[0msock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtls_in_tls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserver_hostname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mserver_hostname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m         )\n",
      "\u001B[0;32m~/PycharmProjects/numerai_tree_regression/venv/tree_regression/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001B[0m in \u001B[0;36m_ssl_wrap_socket_impl\u001B[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001B[0m\n\u001B[1;32m    491\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mserver_hostname\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 493\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mssl_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrap_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mserver_hostname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mserver_hostname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    494\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    495\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mssl_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrap_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001B[0m in \u001B[0;36mwrap_socket\u001B[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[1;32m    498\u001B[0m         \u001B[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    499\u001B[0m         \u001B[0;31m# ctx._wrap_socket()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 500\u001B[0;31m         return self.sslsocket_class._create(\n\u001B[0m\u001B[1;32m    501\u001B[0m             \u001B[0msock\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msock\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    502\u001B[0m             \u001B[0mserver_side\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mserver_side\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001B[0m in \u001B[0;36m_create\u001B[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[1;32m   1038\u001B[0m                         \u001B[0;31m# non-blocking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1039\u001B[0m                         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_handshake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mOSError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001B[0m in \u001B[0;36mdo_handshake\u001B[0;34m(self, block)\u001B[0m\n\u001B[1;32m   1307\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0.0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mblock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1308\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msettimeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1309\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_handshake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1310\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1311\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msettimeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(training_data.shape)\n",
    "training_data.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 40,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/s4/hqpk71s51lq88685j836lbjw0000gn/T/ipykernel_83210/2030501680.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtraining_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_data' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(validation_data.shape)\n",
    "validation_data.head()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# debug \n",
    "if CFG.DEBUG:\n",
    "    training_data = training_data.sample(10000, random_state=SEED)\n",
    "    logger.info('reduced train for debug')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Features, Target\n",
    "We use all the features for now. As for a target, you specify which one to use before in the config section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# features\n",
    "features = training_data.columns[training_data.columns.str.startswith('feature')].values.tolist()\n",
    "logger.info('{:,} features.'.format(len(features)))\n",
    "logger.info(features)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# target\n",
    "sns.jointplot(data=training_data, x=\"target_kazutsugi\", y=\"target_nomi\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling\n",
    "Here we build a simple MLP using tensorflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# set seed to reproduce the result\n",
    "def seed_everything(seed : int) -> NoReturn :    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(CFG.SEED) "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# my default NN hyperparameters\n",
    "params = {\n",
    "    'input_dim': len(features),\n",
    "    'input_dropout': 0.0,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_units': 256,\n",
    "    'hidden_activation': 'relu',\n",
    "    'lr': 1e-03,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 192\n",
    "}\n",
    "logger.info('default NN params:')\n",
    "logger.info(params)\n",
    "\n",
    "def create_model(params=params):\n",
    "    \"\"\"\n",
    "    baseline model\n",
    "    \"\"\"\n",
    "\n",
    "    # NN model architecture\n",
    "    n_neuron = params['hidden_units']\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(params['input_dim'], ))\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    x = tf.keras.layers.Dense(n_neuron, activation=params['hidden_activation'])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(params['dropout'])(x)\n",
    "\n",
    "    # stack more layers\n",
    "    for i in np.arange(params['hidden_layers'] - 1):\n",
    "        x = tf.keras.layers.Dense(n_neuron // (2 * (i+1)), activation=params['hidden_activation'])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(params['dropout'])(x)\n",
    "\n",
    "    out = tf.keras.layers.Dense(1, activation='linear', name = 'out')(x)\n",
    "        \n",
    "    # compile\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    opt = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tuning_model(hp, params=params):\n",
    "    \"\"\"\n",
    "    model tuning with KerasTuner\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(params['input_dim'], ))\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 512, step=128), activation=params['hidden_activation'])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1, default=0.5))(x)\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        x = tf.keras.layers.Dense(hp.Int(f'num_units_{i+2}', 128, 512, step=128))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i+2}', 0.0, 0.5, step=0.1, default=0.5))(x)\n",
    "        \n",
    "    # output\n",
    "    out = tf.keras.layers.Dense(1, activation='linear', name = 'out')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n",
    "   \n",
    "    # compile\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    opt = tf.keras.optimizers.Adam(lr=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['mse'])\n",
    "    \n",
    "    return model\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning (or not)\n",
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. \n",
    "\n",
    "Here we use the **BayesianOptimization** tuner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# create a dataset for NN based on a task\n",
    "train_set = {'X': training_data[features].values, 'y': training_data[CFG.TARGET].values}\n",
    "valid_set = {'X': validation_data[features].values, 'y': validation_data[CFG.TARGET].values}"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if CFG.TUNING:\n",
    "    # define a custom tuner to tune the batch size\n",
    "    class MyTuner(kt.tuners.BayesianOptimization):\n",
    "      def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 128, 8192, step=128)\n",
    "#         kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "    # instantiate KerasTuner\n",
    "    model_ft = lambda hp: tuning_model(hp, params)\n",
    "    tuner = MyTuner(\n",
    "        hypermodel=model_ft,\n",
    "        objective=kt.Objective('val_loss', direction='min'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=20,\n",
    "        overwrite=True)\n",
    "    \n",
    "    # perform tuning\n",
    "    tuner.search(train_set['X'], train_set['y'], \n",
    "                 epochs = 8, validation_data = (valid_set['X'], valid_set['y']))\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "    \n",
    "    # Build the model with the optimal hyperparameters and train it on the data\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    \n",
    "    # disp best params\n",
    "    logger.info('Best hyperparameters:')\n",
    "    logger.info(best_hps.values)\n",
    "else:\n",
    "    # baseline (no tuning)\n",
    "    model = create_model(params)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tf.keras.utils.plot_model(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit with the best model\n",
    "Let's use the model with the best hyperparameters (if tuned). \n",
    "\n",
    "Here I use **Early Stopping** such that the model does not overfit. \n",
    "\n",
    "As a learning rate scheduler, I use **ReduceLROnPlateau**.\n",
    "\n",
    "I do not submit the model prediction in this notebook, but to make the submission process a bit easier, I save the entire model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# callbacks\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, monitor='val_loss')\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='mybestweight.hdf5', save_weights_only=True, \n",
    "#                                                                verbose=0, monitor='val_loss', save_best_only=True)\n",
    "nn_callbacks = [es, lr_scheduler, ]\n",
    "\n",
    "# fit\n",
    "history = model.fit(train_set['X'], train_set['y'], callbacks=nn_callbacks, \n",
    "                    verbose=2, epochs=params['epochs'], batch_size=best_hps.values['batch_size'], \n",
    "                    validation_data=(valid_set['X'], valid_set['y'])) \n",
    "\n",
    "# you can load the model for inference\n",
    "# model = tf.keras.models.load_model(CFG.OUTPUT_DIR + 'saved_model/my_model')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save the entire model\n",
    "model.save(CFG.OUTPUT_DIR + 'my_model.h5')\n",
    "logger.info('Entire model saved!')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# summarize history for loss\n",
    "with plt.xkcd(): # just for fun\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# prediction for valid\n",
    "pred = model.predict(valid_set['X']).ravel()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.hist(pred, alpha=0.4, label='prediction')\n",
    "plt.hist(valid_set['y'], alpha=0.4, label='actual target')\n",
    "plt.legend(frameon=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validation Score\n",
    "This is what we care about! Here we compute Numerai-related scores except for MMC (which we cannot compute as we don't have a meta-model prediction).\n",
    "\n",
    "Note that we split the validation set into the two parts and compute scores on the corresponding eras. This is because the first half validation eras are easy to predict, whereas the last half are hard. It is better to see that our model performs well on the both periods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# naming conventions\n",
    "PREDICTION_NAME = 'prediction'\n",
    "TARGET_NAME = CFG.TARGET\n",
    "EXAMPLE_PRED = 'example_prediction'\n",
    "\n",
    "# ---------------------------\n",
    "# Functions\n",
    "# ---------------------------\n",
    "def valid4score(valid : pd.DataFrame, pred : np.ndarray, load_example: bool=True, save : bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate new valid pandas dataframe for computing scores\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid : pd.DataFrame extracted from tournament data (data_type='validation')\n",
    "    \n",
    "    \"\"\"\n",
    "    valid_df = valid.copy()\n",
    "    valid_df['prediction'] = pd.Series(pred).rank(pct=True, method=\"first\")\n",
    "    valid_df.rename(columns={TARGET_NAME: 'target'}, inplace=True)\n",
    "    \n",
    "    if load_example:\n",
    "        valid_df[EXAMPLE_PRED] = pd.read_csv(EXP_DIR + 'valid_df.csv')['prediction'].values\n",
    "    \n",
    "    if save==True:\n",
    "        valid_df.to_csv(OUTPUT_DIR + 'valid_df.csv', index=False)\n",
    "        logger.info('Validation dataframe saved!')\n",
    "    \n",
    "    return valid_df\n",
    "\n",
    "def compute_corr(valid_df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute rank correlation\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.corrcoef(valid_df[\"target\"], valid_df['prediction'])[0, 1]\n",
    "\n",
    "def compute_max_drawdown(validation_correlations : pd.Series):\n",
    "    \"\"\"\n",
    "    Compute max drawdown\n",
    "    \n",
    "    :INPUT:\n",
    "    - validation_correaltions : pd.Series\n",
    "    \"\"\"\n",
    "    \n",
    "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100, min_periods=1).max()\n",
    "    daily_value = (validation_correlations + 1).cumprod()\n",
    "    max_drawdown = -(rolling_max - daily_value).max()\n",
    "    \n",
    "    return max_drawdown\n",
    "\n",
    "def compute_val_corr(valid_df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute rank correlation for valid periods\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
    "    \"\"\"\n",
    "    \n",
    "    # all validation\n",
    "    correlation = compute_corr(valid_df)\n",
    "    logger.info(\"rank corr = {:.4f}\".format(correlation))\n",
    "    return correlation\n",
    "    \n",
    "def compute_val_sharpe(valid_df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute sharpe ratio for valid periods\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
    "    \"\"\"\n",
    "    # all validation\n",
    "    d = valid_df.groupby('era')[['target', 'prediction']].corr().iloc[0::2,-1].reset_index()\n",
    "    me = d['prediction'].mean()\n",
    "    sd = d['prediction'].std()\n",
    "    max_drawdown = compute_max_drawdown(d['prediction'])\n",
    "    logger.info('sharpe ratio = {:.4f}, corr mean = {:.4f}, corr std = {:.4f}, max drawdown = {:.4f}'.format(me / sd, me, sd, max_drawdown))\n",
    "    \n",
    "    return me / sd, me, sd, max_drawdown\n",
    "    \n",
    "def feature_exposures(valid_df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute feature exposure\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
    "    \"\"\"\n",
    "    feature_names = [f for f in valid_df.columns\n",
    "                     if f.startswith(\"feature\")]\n",
    "    exposures = []\n",
    "    for f in feature_names:\n",
    "        fe = spearmanr(valid_df['prediction'], valid_df[f])[0]\n",
    "        exposures.append(fe)\n",
    "    return np.array(exposures)\n",
    "\n",
    "def max_feature_exposure(fe : np.ndarray):\n",
    "    return np.max(np.abs(fe))\n",
    "\n",
    "def feature_exposure(fe : np.ndarray):\n",
    "    return np.sqrt(np.mean(np.square(fe)))\n",
    "\n",
    "def compute_val_feature_exposure(valid_df : pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute feature exposure for valid periods\n",
    "    \n",
    "    :INPUT:\n",
    "    - valid_df : pd.DataFrame where at least 2 columns ('prediction' & 'target') exist\n",
    "    \"\"\"\n",
    "    # all validation\n",
    "    fe = feature_exposures(valid_df)\n",
    "    fe1, fe2 = feature_exposure(fe), max_feature_exposure(fe)\n",
    "    logger.info('feature exposure = {:.4f}, max feature exposure = {:.4f}'.format(fe1, fe2))\n",
    "     \n",
    "    return fe1, fe2\n",
    "\n",
    "# to neutralize a column in a df by many other columns\n",
    "def neutralize(df, columns, by, proportion=1.0):\n",
    "    scores = df.loc[:, columns]\n",
    "    exposures = df[by].values\n",
    "\n",
    "    # constant column to make sure the series is completely neutral to exposures\n",
    "    exposures = np.hstack(\n",
    "        (exposures,\n",
    "         np.asarray(np.mean(scores)) * np.ones(len(exposures)).reshape(-1, 1)))\n",
    "\n",
    "    scores = scores - proportion * exposures.dot(\n",
    "        np.linalg.pinv(exposures).dot(scores))\n",
    "    return scores / scores.std()\n",
    "\n",
    "\n",
    "# to neutralize any series by any other series\n",
    "def neutralize_series(series, by, proportion=1.0):\n",
    "    scores = series.values.reshape(-1, 1)\n",
    "    exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "    exposures = np.hstack(\n",
    "        (exposures,\n",
    "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
    "\n",
    "    correction = proportion * (exposures.dot(\n",
    "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
    "    corrected_scores = scores - correction\n",
    "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "    return neutralized\n",
    "\n",
    "\n",
    "def unif(df):\n",
    "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "    return pd.Series(x, index=df.index)\n",
    "\n",
    "def get_feature_neutral_mean(df):\n",
    "    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
    "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
    "                                          feature_cols)[PREDICTION_NAME]\n",
    "    scores = df.groupby(\"era\").apply(\n",
    "        lambda x: np.corrcoef(x[\"neutral_sub\"].rank(pct=True, method=\"first\"), x[TARGET_NAME])).mean()\n",
    "    return np.mean(scores)\n",
    "\n",
    "def compute_val_mmc(valid_df : pd.DataFrame):    \n",
    "    # MMC over validation\n",
    "    mmc_scores = []\n",
    "    corr_scores = []\n",
    "    for _, x in valid_df.groupby(\"era\"):\n",
    "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
    "                                   pd.Series(unif(x[EXAMPLE_PRED])))\n",
    "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
    "        corr_scores.append(np.corrcoef(unif(x[PREDICTION_NAME]).rank(pct=True, method=\"first\"), x[TARGET_NAME]))\n",
    "\n",
    "    val_mmc_mean = np.mean(mmc_scores)\n",
    "    val_mmc_std = np.std(mmc_scores)\n",
    "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
    "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
    "\n",
    "    logger.info(\"MMC Mean = {:.6f}, MMC Std = {:.6f}, CORR+MMC Sharpe = {:.4f}\".format(val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe))\n",
    "\n",
    "    # Check correlation with example predictions\n",
    "    corr_with_example_preds = np.corrcoef(valid_df[EXAMPLE_PRED].rank(pct=True, method=\"first\"),\n",
    "                                          valid_df[PREDICTION_NAME].rank(pct=True, method=\"first\"))[0, 1]\n",
    "    logger.info(\"Corr with example preds: {:.4f}\".format(corr_with_example_preds))\n",
    "    \n",
    "    return val_mmc_mean, val_mmc_std, corr_plus_mmc_sharpe, corr_with_example_preds\n",
    "    \n",
    "def score_summary(valid_df : pd.DataFrame):\n",
    "    score_df = {}\n",
    "    \n",
    "    try:\n",
    "        score_df['correlation'] = compute_val_corr(valid_df)\n",
    "    except:\n",
    "        print('ERR: computing correlation')\n",
    "    try:\n",
    "        score_df['corr_sharpe'], score_df['corr_mean'], score_df['corr_std'], score_df['max_drawdown'] = compute_val_sharpe(valid_df)\n",
    "    except:\n",
    "        print('ERR: computing sharpe')\n",
    "    try:\n",
    "        score_df['feature_exposure'], score_df['max_feature_exposure'] = compute_val_feature_exposure(valid_df)\n",
    "    except:\n",
    "        print('ERR: computing feature exposure')\n",
    "    try:\n",
    "        score_df['mmc_mean'], score_df['mmc_std'], score_df['corr_mmc_sharpe'], score_df['corr_with_example_xgb'] = compute_val_mmc(valid_df)\n",
    "    except:\n",
    "        print('ERR: computing MMC')\n",
    "    \n",
    "    return pd.DataFrame.from_dict(score_df, orient='index')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "valid_df = valid4score(validation_data, pred, load_example=False, save=False)\n",
    "\n",
    "# scores\n",
    "score_df = pd.DataFrame()\n",
    "print('------------------')\n",
    "print('ALL:')\n",
    "print('------------------')\n",
    "all_ = score_summary(valid_df).rename(columns={0: 'all'})\n",
    "\n",
    "print('------------------')\n",
    "print('VALID 1:')\n",
    "print('------------------')\n",
    "val1_ = score_summary(valid_df.query('era < 150')).rename(columns={0: 'val1'})\n",
    "\n",
    "print('------------------')\n",
    "print('VALID 2:')\n",
    "print('------------------')\n",
    "val2_ = score_summary(valid_df.query('era > 150')).rename(columns={0: 'val2'})"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# scores\n",
    "score_df = pd.concat([all_, val1_, val2_], axis=1)\n",
    "score_df.style.background_gradient(cmap='viridis', axis=0)\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}